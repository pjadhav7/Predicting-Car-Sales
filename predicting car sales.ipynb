{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from ipywidgets import interact\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load and read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"imports-85.data\"\n",
    "df = pd.read_csv(data,header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the header for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\",\"num-of-doors\",\"body-style\",\"drive-wheels\",\n",
    "           \"engine-location\",\"wheel-base\",\"length\",\"width\",\"height\",\"curb-weight\",\"engine-type\",\"num-of-cylinders\",\n",
    "           \"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\"peak-rpm\",\"city-mpg\",\"highway-mpg\",\n",
    "           \"price\"]\n",
    "df.columns = headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop/ Remove rows that contains missing values like NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset = [\"price\"], axis=0, inplace=True)\n",
    "df.dropna(subset = [\"peak-rpm\"], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert datatype from object to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_price = pd.to_numeric(df[\"price\"],errors='coerce')\n",
    "df[\"price\"] = vals_price.fillna(vals_price.mean())\n",
    "\n",
    "vals_peakRpm = pd.to_numeric(df[\"peak-rpm\"],errors='coerce')\n",
    "df[\"peak-rpm\"] = vals_peakRpm.fillna(vals_peakRpm.mean())\n",
    "\n",
    "vals_hp = pd.to_numeric(df[\"horsepower\"],errors='coerce')\n",
    "df[\"horsepower\"] = vals_hp.fillna(vals_hp.mean())\n",
    "\n",
    "vals = pd.to_numeric(df[\"normalized-losses\"],errors='coerce')\n",
    "df[\"normalized-losses\"] = vals.fillna(vals.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert mpg to L/100kms and rename the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"city-mpg\"] = 235/df[\"city-mpg\"]\n",
    "df.rename(columns={\"city_mpg\": \"city-L/100km\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"length\"] = (df[\"length\"]-df[\"length\"].mean())/df[\"length\"].std()\n",
    "df[\"width\"] = (df[\"width\"]-df[\"width\"].mean())/df[\"width\"].std()\n",
    "df[\"height\"] = (df[\"height\"]-df[\"height\"].mean())/df[\"height\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(min(df[\"price\"]),max(df[\"price\"]),4)\n",
    "group_names = [\"Low\", \"Medium\",\"High\"]\n",
    "df[\"price-binned\"] = pd.cut(df[\"price\"],bins,labels=group_names,include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert categorical values to quantitative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df['fuel-type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summarize the categorical data using value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_wheels_counts=df[\"drive-wheels\"].value_counts()\n",
    "drive_wheels_counts.rename(columns={\"drive-wheels\":\"value_counts\"}, inplace=True)\n",
    "drive_wheels_counts.index.name = 'drive-wheels'\n",
    "print(drive_wheels_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boxplot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"drive-wheels\",y=\"price\",data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each observation in a scatter plot is represented as a point. This plot shows the relationship between two variables:\n",
    "The predictor variable: is the variable that you are using to predict an outcome. In this\n",
    "case, our predictor variable is the engine size.\n",
    "The target variable: is the variable that you are trying to predict. In this case, our\n",
    "target variable is the price, since this would be the outcome.\n",
    "In this case, we will thus plot the engine size on the x-axis and the price on the y-axis.\n",
    "We are using the Matplotlib function “scatter” here, taking in x and a y variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[\"engine-size\"]\n",
    "x=df[\"price\"]\n",
    "plt.scatter(x,y)\n",
    "plt.title(\"Scatterplot of Engine Size vs Price\")\n",
    "plt.xlabel(\"Engine Size\")\n",
    "plt.ylabel(\"Price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using groupby to find the average price of vehicles and\n",
    "observe how they differ between different types of “body styles” and “drive wheels”\n",
    "variables.\n",
    "To do this, we first pick out the three data columns we are interested in, which is done\n",
    "in the first line of code.\n",
    "We then group the reduced data according to ‘drive wheels’ and ‘body style’ in\n",
    "the second line.\n",
    "Since we are interested in knowing how the average price differs across the board, we\n",
    "can take the mean of each group and append this bit at the very end of line 2. \n",
    "We can see that, according to our data, rear wheel drive convertibles and rear wheel drive hardtops have the highest value, while four wheel drive hatchbacks have the lowest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[['drive-wheels','body-style','price']]\n",
    "df_grp = df_test.groupby(['drive-wheels','body-style'], as_index=False).mean()\n",
    "df_grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transform the above table to a pivot table by using the pivot\n",
    "method.\n",
    "In the previous table, both ‘drive wheels’ and ‘body style’ were listed in columns.\n",
    "A pivot table has one variable displayed along the columns and the other variable displayed\n",
    "along the rows.\n",
    "Just with one line of code and by using the pandas pivot method, we can pivot the “body\n",
    "style” variable so it is displayed along the columns and the “drive wheels” will\n",
    "be displayed along the rows.\n",
    "The price data now becomes a rectangular grid, which is easier to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df_grp.pivot(index='drive-wheels', columns='body-style')\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat map takes a rectangular grid of data and assigns a color intensity based on the\n",
    "data value at the grid points.\n",
    "It is a great way to plot the target variable over multiple variables and through this get\n",
    "visual clues of the relationship between these variables and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(df_pivot, cmap='RdBu')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Variance (ANOVA)\n",
    "In the first line we extract the make and price data.\n",
    "Then, we'll group the data by different makes.\n",
    "The ANOVA test can be performed in Python using the f_oneway method as the built-in\n",
    "function of the Scipy package.\n",
    "We pass in the price data of the two car make groups that we want to compare and it calculates\n",
    "the ANOVA results.\n",
    "The prices between Hondas and Subarus are not significantly different, as the F-test\n",
    "score is less than 1 and p-value is larger than 0.05.\n",
    "We can do the same for Honda and Jaguar.\n",
    "The prices between Hondas and Jaguars are significantly different, since the F-score\n",
    "is very large (F = 401) and the p-value is larger than 0.05.\n",
    "All in all, we can say that there is a strong correlation between a categorical variable\n",
    "and other variables, if the ANOVA test gives us a large F-test value and a small p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anova = df[[\"make\", \"price\"]]\n",
    "grouped_anova = df_anova.groupby([\"make\"])\n",
    "anova_results_1 = stats.f_oneway(grouped_anova.get_group(\"honda\")[\"price\"], grouped_anova.get_group(\"subaru\")[\"price\"])\n",
    "anova_results_2 = stats.f_oneway(grouped_anova.get_group(\"honda\")[\"price\"], grouped_anova.get_group(\"jaguar\")[\"price\"])\n",
    "print(anova_results_1)\n",
    "print(anova_results_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Plot provides good estimate of:\n",
    "\n",
    "1. The relationship betweeen two variables.\n",
    "2. The strength of the correlation\n",
    "3. The direction of the relationship (positive or negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the correlation between engine size and price. This time we'll visualize these two variables using\n",
    "a scatter plot and an added linear line called a regression line, which indicates the relationship between the two.\n",
    "The main goal of this plot is to see whether the engine size has any impact on the price.\n",
    "In this example, you can see that the straight line through the data points is very\n",
    "steep which shows that there's a positive linear relationship between the two variables.\n",
    "With increase in values of engine size, values of price go up as well and the slope of the line is positive. So there is a positive correlation between engine size and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=\"engine-size\", y=\"price\", data = df)\n",
    "plt.ylim(0,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example, now let's look at the relationship between highway miles per gallon to see its impact on the car price. As we can see in this plot, when highway miles per gallon value goes up the value price goes down. Therefore there is a negative linear relationship between highway miles per gallon and price. Although this relationship is negative the slope of the line is steep which means that the highway miles per gallon is still a good predictor of price. These two variables are said to have a negative correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=\"highway-mpg\", y=\"price\", data = df)\n",
    "plt.ylim(0,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have an example of a weak correlation. For example, both low peak RPM and high values of peak RPM have low and high prices. Therefore, we cannot use RPM to predict the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=\"peak-rpm\", y=\"price\", data = df)\n",
    "plt.ylim(0,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the “residplot” function. The first parameter is a series of dependent variable or feature. The second parameter is a Series of dependent variable or target. We see in this case the Residuals have a curvature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(df['highway-mpg'], df['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson correlation method will gives two values: the correlation coefficient and the P-value.For the correlation coefficient,\n",
    "a value close to 1 implies a large positive correlation, while a value close to negative 1 implies a large negative correlation, and a value close to zero implies no correlation between the variables. Next, the P-value will tell us how\n",
    "certain we are about the correlation that we calculated. For the P-value, a value less than .001 gives us a strong certainty about the correlation coefficient that we calculated. A value between.001 and.05 gives us moderate certainty. A value between.05 and.1 will give us a weak certainty. And a P-value larger than.1 will give us no certainty of correlation at all.\n",
    "We can say that there is a strong correlation when the correlation coefficient is close to 1 or negative 1, and the P-value is less than.001. The following plot shows data with different correlation values. In this example, we want to look at the correlation between the variable's horsepower and car price. We can see that the correlation coefficient is approximately.8, and this is close to 1. So there is a strong positive correlation. We can also see that the P-value is very small, much smaller than.001. And so we can conclude that we are certain about the strong positive correlation. Taking all variables into account,\n",
    "we can now create a heatmap that indicates the correlation between each of the variables with one another. The color scheme indicates the Pearson correlation coefficient, indicating the strength of the correlation between two variables. We can see a diagonal line with a dark red color, indicating that all the values on this diagonal are highly correlated. This makes sense because when you look closer, the values on the diagonal are the correlation of all variables with themselves, which will be always 1. This correlation heatmap gives us a good overview of how the different variables are related to one another and,\n",
    "most importantly, how these variables are related to price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coef, p_value = stats.pearsonr(df[\"horsepower\"], df[\"price\"])\n",
    "print(pearson_coef)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Linear Regression Object using the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the predictor variable and the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['highway-mpg']\n",
    "Y = df['price']\n",
    "X1 = df['highway-mpg']\n",
    "Y1 = df['price']\n",
    "X = X.values.reshape(-1,1)\n",
    "Y = Y.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use lm.fit(X,Y) to fit the model, i.e. fine the parameters b0 and b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = lm.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intercept b0 of a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slope b1 of a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distribution plot counts the predicted value versus the actual value. These plots are extremely useful for visualizing models with more than one independent variable or feature. We examine the vertical axis.\n",
    "- We then count and plot the number of predicted points that are approximately equal to one.\n",
    "- We then count and plot the number of predicted points that are approximately equal to two.\n",
    "- We repeat the process for predicted points that are approximately equal to three.\n",
    "Then we repeat the process for the target values.\n",
    "In this case, all the target values are approximately equal to two. The values of the targets and predicted values are continuous. A histogram is for discrete values. Therefore pandas will convert them to a distribution.\n",
    "The vertical access is scaled to make the area under the distribution equal to one.\n",
    "This is an example of using a distribution plot.\n",
    "The dependent variable or feature is price.\n",
    "The fitted values that result from the model are in blue.\n",
    "The actual values are in red.\n",
    "We see the predicted values for prices in the range from $40 000 to $50 000 are inaccurate.\n",
    "The prices in the region form $10 000 to $20 000 are much closer to the target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = sns.distplot(df['price'], hist=False, color = \"r\", label=\"Actual Value\")\n",
    "sns.distplot(Yhat, hist=False, color = \"b\", label = \"Fitted Values\", ax= ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the R-squared value by using the score() method, in the linear regression object. From the value that we get from the below example, we can say that approximately 47.68% of the variation of the price is explained by this simple linear model. The R-square value is usually between 0 and 1, if the value is a negative number then it can be due to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df['highway-mpg'].values.reshape(-1,1)\n",
    "Y2 = df['price'].values.reshape(-1,1)\n",
    "linReg = LinearRegression()\n",
    "linReg.fit(X2, Y2)\n",
    "linReg.score(X2, Y2,sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to find out what the price would be for a car that has a highway-mpg of 30. Plugging this value into the predict() method, gives us a resulting price of $13,799.86. This seems to make sense, for example, the value is not negative, extremely high or extremely low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = df['highway-mpg'].values.reshape(-1,1)\n",
    "Y3 = df['price'].values.reshape(-1,1)\n",
    "linReg = LinearRegression()\n",
    "linReg.fit(X3, Y3)\n",
    "linReg.predict([[30]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Evaluation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular function in the sci-kit learn package for splitting datasets is the \"train test split\" function. This function randomly splits a dataset into training and testing subsets. The output is an array: \n",
    " * \"x_train\" and \"y_train\", the subsets for training.\n",
    " * \"x_test\" and \"y_test\",the subsets for testing. \n",
    "<br>In this case, the \"test size\" percentage of the data for the testing set. Here it is 15%. The random state is a random seed for random dataset splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Functions for plotting</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):\n",
    "    width = 12\n",
    "    height = 10\n",
    "    plt.figure(figsize=(width, height))\n",
    "\n",
    "    ax1 = sns.distplot(RedFunction, hist=False, color=\"r\", label=RedName)\n",
    "    ax2 = sns.distplot(BlueFunction, hist=False, color=\"b\", label=BlueName, ax=ax1)\n",
    "\n",
    "    plt.title(Title)\n",
    "    plt.xlabel('Price (in dollars)')\n",
    "    plt.ylabel('Proportion of Cars')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PollyPlot(xtrain, xtest, y_train, y_test, lr,poly_transform):\n",
    "    width = 12\n",
    "    height = 10\n",
    "    plt.figure(figsize=(width, height))\n",
    "    \n",
    "    \n",
    "    #training data \n",
    "    #testing data \n",
    "    # lr:  linear regression object \n",
    "    #poly_transform:  polynomial transformation object \n",
    " \n",
    "    xmax=max([xtrain.values.max(), xtest.values.max()])\n",
    "\n",
    "    xmin=min([xtrain.values.min(), xtest.values.min()])\n",
    "\n",
    "    x=np.arange(xmin, xmax, 0.1)\n",
    "\n",
    "\n",
    "    plt.plot(xtrain, y_train, 'ro', label='Training Data')\n",
    "    plt.plot(xtest, y_test, 'go', label='Test Data')\n",
    "    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label='Predicted Function')\n",
    "    plt.ylim([-10000, 60000])\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = df['price']\n",
    "x_data = df.drop('price', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and testing data using the function train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.15, random_state=1)\n",
    "print(\"number of test samples :\", x_test.shape[0])\n",
    "print(\"number of training samples:\",x_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a Linear Regression object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we fit the model using the feature horsepower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre.fit(x_train[['horsepower']], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Calculate the R^2 on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre.score(x_train[['horsepower']], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you do not have sufficient testing data; as a result, you may want to perform Cross-validation. Let's go over several methods that you can use for Cross-validation. <br> We input the object, the feature in this case ' horsepower', the target data (y_data). The parameter 'cv' determines the number of folds; in this case 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rcross = cross_val_score(lre, x_data[['horsepower']], y_data, cv=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default scoring is R^2; each element in the array has the average R^2 value in the fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rcross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the average and standard deviation of our estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean of the folds are\", Rcross.mean(), \" and the standard deviation is\", Rcross.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'cross_val_predict' function splits up the data into the specified number of folds, using one fold to get a prediction while the rest of the folds are used as test data. <br> We input the object, the feature in this case 'horsepower' , the target data y_data. The parameter 'cv' determines the number of folds; in this case 4. We can produce an output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = cross_val_predict(lre, x_data[['horsepower']], y_data, cv=4)\n",
    "yhat[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Overfitting, Underfitting and Model Selection</h3>\n",
    "\n",
    "<p>It turns out that the test data sometimes referred to as the out of sample data is a much better measure of how well your model performs in the real world.  One reason for this is overfitting; It turns out these differences are more apparent in Multiple Linear Regression and Polynomial Regression so we will explore overfitting in that context.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create Multiple linear regression objects and train the model using 'horsepower', 'curb-weight', 'engine-size' and 'highway-mpg' as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction using training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train = lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n",
    "yhat_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction using test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_test = lr.predict(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n",
    "yhat_test[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing some model evaluation using our training and testing data separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title = 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution'\n",
    "DistributionPlot(y_train, yhat_train, \"Actual Values (Train)\", \"Predicted Values (Train)\", Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'\n",
    "DistributionPlot(y_test,yhat_test,\"Actual Values (Test)\",\"Predicted Values (Test)\",Title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing Figure 1 and Figure 2; it is evident the distribution of the test data in Figure 1 is much better at fitting the data. This difference in Figure 2 is apparent where the ranges are from 5000 to 15 000. This is where the distribution shape is exceptionally different.<br> Let's perform polynomial regression to check whether it exhibits a drop in the prediction accuracy when analysing the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Overfitting</h4>\n",
    "<p>Overfitting occurs when the model fits the noise, not the underlying process. Therefore when testing your model using the test-set, your model does not perform as well as it is modelling noise, not the underlying process that generated the relationship. Let's create a degree 5 polynomial model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use 55 percent of the data for testing and the rest for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a degree 5 polynomial transformation on the feature 'horse power'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PolynomialFeatures(degree=5)\n",
    "x_train_pr = pr.fit_transform(x_train[['horsepower']])\n",
    "x_test_pr = pr.fit_transform(x_test[['horsepower']])\n",
    "pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a linear regression model \"poly\" and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = LinearRegression()\n",
    "poly.fit(x_train_pr, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the output of our model using the method \"predict.\" then assign the values to \"yhat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = poly.predict(x_test_pr)\n",
    "yhat[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first five predicted values and compare it to the actual targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted values:\", yhat[0:4])\n",
    "print(\"True values:\", y_test[0:4].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the function \"PollyPlot\" that we defined at the beginning of the lab to display the training data, testing data, and the predicted function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train, y_test, poly,pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A polynomial regression model, red dots represent training data, green dots represent test data, and the blue line represents the model prediction. We see that the estimated function appears to track the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(order, test_data):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_data, random_state=0)\n",
    "    pr = PolynomialFeatures(degree=order)\n",
    "    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n",
    "    x_test_pr = pr.fit_transform(x_test[['horsepower']])\n",
    "    poly = LinearRegression()\n",
    "    poly.fit(x_train_pr,y_train)\n",
    "    PollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train,y_test, poly, pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following interface allows you to experiment with different polynomial orders and different amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(f, order=(0, 6, 1), test_data=(0.05, 0.95, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Ridge regression</h3> <br>\n",
    "Using Ridge Regression we will check how the parameter alpha changes the model. <br>\n",
    "Let's perform a degree two polynomial transformation on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PolynomialFeatures(degree=2)\n",
    "x_train_pr = pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])\n",
    "x_test_pr=pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Ridge regression object, setting the regularization parameter to 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RigeModel=Ridge(alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like regular regression, we can fit the model using the method fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RigeModel.fit(x_train_pr, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = RigeModel.predict(x_test_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the first five predicted samples to our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('predicted:', yhat[0:4])\n",
    "print('test set :', y_test[0:4].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the value of Alfa that minimizes the test error, for example, we can use a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsqu_test = []\n",
    "Rsqu_train = []\n",
    "dummy1 = []\n",
    "ALFA = 10 * np.array(range(0,1000))\n",
    "for alfa in ALFA:\n",
    "    RigeModel = Ridge(alpha=alfa) \n",
    "    RigeModel.fit(x_train_pr, y_train)\n",
    "    Rsqu_test.append(RigeModel.score(x_test_pr, y_test))\n",
    "    Rsqu_train.append(RigeModel.score(x_train_pr, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot out the value of R^2 for different Alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 12\n",
    "height = 10\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "plt.plot(ALFA,Rsqu_test, label='validation data  ')\n",
    "plt.plot(ALFA,Rsqu_train, 'r', label='training Data ')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('R^2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue line represents the R^2 of the test data, and the red line represents the R^2 of the training data. The x-axis represents the different values of Alfa.<br>\n",
    "The red line in figure represents the  R^2 of the test data, as Alpha increases the R^2 decreases; therefore as Alfa increases the model performs worse on the test data.  The blue line represents the R^2 on the validation data, as the value for Alfa increases the R^2 decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Grid Search</h3> <br>\n",
    "The term Alfa is a hyperparameter, sklearn has the class  <b>GridSearchCV</b> to make the process of finding the best hyperparameter simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary of parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters1= [{'alpha': [0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ridge regions object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RR=Ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ridge grid search object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid1 = GridSearchCV(RR, parameters1,cv=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid1.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object finds the best parameter values on the validation data. We can obtain the estimator with the best parameters and assign it to the variable BestRR as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestRR=Grid1.best_estimator_\n",
    "BestRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now test the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestRR.score(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
